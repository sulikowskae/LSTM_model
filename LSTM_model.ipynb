{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_y_4sjrBAB1"
      },
      "source": [
        "##Loading text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LDNMONNWFHw"
      },
      "source": [
        "path = '/content/drive/MyDrive/'\n",
        "file = open(path + 'fashion_corpus_clean.txt', 'r')\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9xpW9_qV9Qj",
        "outputId": "a95d28ce-66c2-438d-e798-c5757a4cdaf1"
      },
      "source": [
        "tokens = text[:500000].split() #part of the whole dataset\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "print('Number of tokens: ', len(tokens))\n",
        "print('Number of unique tokens: ', len(set(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:  57075\n",
            "Number of unique tokens:  14261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY6kNuM8BtDL"
      },
      "source": [
        "##Creating sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVOM1FxIV9VV",
        "outputId": "4894a6e3-4954-4a81-bfdb-8f741e2fa051"
      },
      "source": [
        "length = 20 + 1 #length of sequences\n",
        "\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\tseq = tokens[i-length:i]\n",
        "\tline = ' '.join(seq)\n",
        "\tsequences.append(line)\n",
        "\n",
        "print('number of sequences: ', len(sequences))\n",
        "\n",
        "out_filename = 'fashion_sequences.txt'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of sequences:  57054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK_wOmIlaMZi"
      },
      "source": [
        "#save sequences in text file\n",
        "data = '\\n'.join(sequences)\n",
        "file = open(path+out_filename, 'w')\n",
        "file.write(data)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwXnyKTqB_ov"
      },
      "source": [
        "##Creating model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzjpG4GEV9Yi"
      },
      "source": [
        "from numpy import array\n",
        "import numpy as np\n",
        "from pickle import dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwNQqeB8aD8z"
      },
      "source": [
        "path = '/content/drive/MyDrive/'\n",
        "file = open(path + 'fashion_sequences.txt', 'r')\n",
        "doc = file.read()\n",
        "file.close()\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALrhI6uMZLtR"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMzwwkumZaM_"
      },
      "source": [
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjguR11R2msH"
      },
      "source": [
        "The first layer is an **Embedding layer** which stores one vector per word. It converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training, words with similar meanings often have similar vectors.\n",
        "\n",
        "**LSTM layer** (Long Short-Term Memory). Enables the model to learn what information to store in long term memory and what to get rid of.\n",
        "\n",
        "\n",
        "**Dropout layer** prevents model from overfitting to training data.\n",
        "\n",
        "**Flatten layer** reshapes tensor to have the shape that is equal to the number of elements contained in tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pzWD7epZfem",
        "outputId": "a6dd02cd-6eac-48f5-894b-23358298c3df"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "#model.add(Dropout(0.4))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 100)           1426100   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 20, 100)           80400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 50)            30200     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 20, 32)            10624     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                10256     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 14261)             242437    \n",
            "=================================================================\n",
            "Total params: 1,800,017\n",
            "Trainable params: 1,800,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz70MrFNZgI8",
        "outputId": "fe565f0b-86f8-4618-8a35-cdc5f152dd6a"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, batch_size=64, epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "892/892 [==============================] - 85s 89ms/step - loss: 8.3292 - accuracy: 0.0407\n",
            "Epoch 2/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 8.0168 - accuracy: 0.0419\n",
            "Epoch 3/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 7.9303 - accuracy: 0.0425\n",
            "Epoch 4/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 7.8403 - accuracy: 0.0427\n",
            "Epoch 5/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 7.7397 - accuracy: 0.0429\n",
            "Epoch 6/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 7.6572 - accuracy: 0.0434\n",
            "Epoch 7/200\n",
            "892/892 [==============================] - 77s 87ms/step - loss: 7.5790 - accuracy: 0.0431\n",
            "Epoch 8/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 7.5016 - accuracy: 0.0437\n",
            "Epoch 9/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 7.4146 - accuracy: 0.0435\n",
            "Epoch 10/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 7.3352 - accuracy: 0.0439\n",
            "Epoch 11/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 7.2614 - accuracy: 0.0434\n",
            "Epoch 12/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 7.1889 - accuracy: 0.0439\n",
            "Epoch 13/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 7.1241 - accuracy: 0.0446\n",
            "Epoch 14/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 7.0569 - accuracy: 0.0457\n",
            "Epoch 15/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 6.9912 - accuracy: 0.0464\n",
            "Epoch 16/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 6.9202 - accuracy: 0.0482\n",
            "Epoch 17/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 6.8543 - accuracy: 0.0506\n",
            "Epoch 18/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 6.7894 - accuracy: 0.0523\n",
            "Epoch 19/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 6.7216 - accuracy: 0.0539\n",
            "Epoch 20/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 6.6567 - accuracy: 0.0568\n",
            "Epoch 21/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 6.5941 - accuracy: 0.0592\n",
            "Epoch 22/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 6.5340 - accuracy: 0.0608\n",
            "Epoch 23/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 6.4712 - accuracy: 0.0640\n",
            "Epoch 24/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 6.4114 - accuracy: 0.0656\n",
            "Epoch 25/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 6.3557 - accuracy: 0.0693\n",
            "Epoch 26/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 6.3033 - accuracy: 0.0719\n",
            "Epoch 27/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 6.2568 - accuracy: 0.0757\n",
            "Epoch 28/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 6.2086 - accuracy: 0.0786\n",
            "Epoch 29/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 6.1531 - accuracy: 0.0814\n",
            "Epoch 30/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 6.1108 - accuracy: 0.0846\n",
            "Epoch 31/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 6.0713 - accuracy: 0.0891\n",
            "Epoch 32/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 6.0363 - accuracy: 0.0891\n",
            "Epoch 33/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 5.9895 - accuracy: 0.0946\n",
            "Epoch 34/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.9487 - accuracy: 0.0980\n",
            "Epoch 35/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 5.9209 - accuracy: 0.1016\n",
            "Epoch 36/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.8853 - accuracy: 0.1022\n",
            "Epoch 37/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 5.8534 - accuracy: 0.1047\n",
            "Epoch 38/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 5.8156 - accuracy: 0.1094\n",
            "Epoch 39/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.7790 - accuracy: 0.1112\n",
            "Epoch 40/200\n",
            "892/892 [==============================] - 77s 87ms/step - loss: 5.7581 - accuracy: 0.1129\n",
            "Epoch 41/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.7232 - accuracy: 0.1145\n",
            "Epoch 42/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.7059 - accuracy: 0.1162\n",
            "Epoch 43/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.6571 - accuracy: 0.1199\n",
            "Epoch 44/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.6464 - accuracy: 0.1233\n",
            "Epoch 45/200\n",
            "892/892 [==============================] - 77s 87ms/step - loss: 5.6078 - accuracy: 0.1236\n",
            "Epoch 46/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.5874 - accuracy: 0.1260\n",
            "Epoch 47/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 5.5836 - accuracy: 0.1273\n",
            "Epoch 48/200\n",
            "892/892 [==============================] - 77s 87ms/step - loss: 5.5475 - accuracy: 0.1303\n",
            "Epoch 49/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.5219 - accuracy: 0.1313\n",
            "Epoch 50/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.5006 - accuracy: 0.1335\n",
            "Epoch 51/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.4682 - accuracy: 0.1370\n",
            "Epoch 52/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.4503 - accuracy: 0.1369\n",
            "Epoch 53/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.4275 - accuracy: 0.1399\n",
            "Epoch 54/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.4103 - accuracy: 0.1396\n",
            "Epoch 55/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 5.3953 - accuracy: 0.1418\n",
            "Epoch 56/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.3724 - accuracy: 0.1442\n",
            "Epoch 57/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 5.3644 - accuracy: 0.1450\n",
            "Epoch 58/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.3390 - accuracy: 0.1465\n",
            "Epoch 59/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 5.3284 - accuracy: 0.1486\n",
            "Epoch 60/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.2996 - accuracy: 0.1504\n",
            "Epoch 61/200\n",
            "892/892 [==============================] - 77s 87ms/step - loss: 5.2868 - accuracy: 0.1503\n",
            "Epoch 62/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.2656 - accuracy: 0.1539\n",
            "Epoch 63/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.2503 - accuracy: 0.1530\n",
            "Epoch 64/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.2307 - accuracy: 0.1557\n",
            "Epoch 65/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.2228 - accuracy: 0.1565\n",
            "Epoch 66/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 5.2105 - accuracy: 0.1583\n",
            "Epoch 67/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.1911 - accuracy: 0.1613\n",
            "Epoch 68/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 5.1708 - accuracy: 0.1606\n",
            "Epoch 69/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 5.1537 - accuracy: 0.1613\n",
            "Epoch 70/200\n",
            "892/892 [==============================] - 78s 88ms/step - loss: 5.1323 - accuracy: 0.1649\n",
            "Epoch 71/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 5.1241 - accuracy: 0.1655\n",
            "Epoch 72/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 5.1225 - accuracy: 0.1670\n",
            "Epoch 73/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 5.0957 - accuracy: 0.1677\n",
            "Epoch 74/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 5.0917 - accuracy: 0.1696\n",
            "Epoch 75/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 5.0822 - accuracy: 0.1710\n",
            "Epoch 76/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 5.0571 - accuracy: 0.1711\n",
            "Epoch 77/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 5.0501 - accuracy: 0.1730\n",
            "Epoch 78/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 5.0327 - accuracy: 0.1732\n",
            "Epoch 79/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 5.0208 - accuracy: 0.1769\n",
            "Epoch 80/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 5.0136 - accuracy: 0.1773\n",
            "Epoch 81/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 4.9966 - accuracy: 0.1778\n",
            "Epoch 82/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.9872 - accuracy: 0.1810\n",
            "Epoch 83/200\n",
            "892/892 [==============================] - 76s 86ms/step - loss: 4.9749 - accuracy: 0.1801\n",
            "Epoch 84/200\n",
            "892/892 [==============================] - 76s 86ms/step - loss: 4.9763 - accuracy: 0.1802\n",
            "Epoch 85/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 4.9571 - accuracy: 0.1820\n",
            "Epoch 86/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 4.9367 - accuracy: 0.1835\n",
            "Epoch 87/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.9243 - accuracy: 0.1858\n",
            "Epoch 88/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 4.9244 - accuracy: 0.1850\n",
            "Epoch 89/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 4.9276 - accuracy: 0.1847\n",
            "Epoch 90/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 4.8981 - accuracy: 0.1885\n",
            "Epoch 91/200\n",
            "892/892 [==============================] - 75s 84ms/step - loss: 4.8883 - accuracy: 0.1915\n",
            "Epoch 92/200\n",
            "892/892 [==============================] - 77s 87ms/step - loss: 4.8655 - accuracy: 0.1907\n",
            "Epoch 93/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 4.8715 - accuracy: 0.1925\n",
            "Epoch 94/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.8562 - accuracy: 0.1919\n",
            "Epoch 95/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.8417 - accuracy: 0.1919\n",
            "Epoch 96/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.8270 - accuracy: 0.1960\n",
            "Epoch 97/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.8334 - accuracy: 0.1945\n",
            "Epoch 98/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.8186 - accuracy: 0.1943\n",
            "Epoch 99/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.7966 - accuracy: 0.1985\n",
            "Epoch 100/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.8068 - accuracy: 0.1992\n",
            "Epoch 101/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 4.7786 - accuracy: 0.2004\n",
            "Epoch 102/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.7667 - accuracy: 0.2023\n",
            "Epoch 103/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.7706 - accuracy: 0.2006\n",
            "Epoch 104/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.7626 - accuracy: 0.2014\n",
            "Epoch 105/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.7547 - accuracy: 0.2040\n",
            "Epoch 106/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.7350 - accuracy: 0.2052\n",
            "Epoch 107/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.7202 - accuracy: 0.2067\n",
            "Epoch 108/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 4.7338 - accuracy: 0.2058\n",
            "Epoch 109/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.7094 - accuracy: 0.2085\n",
            "Epoch 110/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.7012 - accuracy: 0.2108\n",
            "Epoch 111/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.6992 - accuracy: 0.2103\n",
            "Epoch 112/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.6797 - accuracy: 0.2130\n",
            "Epoch 113/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.6685 - accuracy: 0.2140\n",
            "Epoch 114/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6698 - accuracy: 0.2121\n",
            "Epoch 115/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6576 - accuracy: 0.2148\n",
            "Epoch 116/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6538 - accuracy: 0.2155\n",
            "Epoch 117/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6387 - accuracy: 0.2178\n",
            "Epoch 118/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6400 - accuracy: 0.2184\n",
            "Epoch 119/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6390 - accuracy: 0.2184\n",
            "Epoch 120/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.6114 - accuracy: 0.2195\n",
            "Epoch 121/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6238 - accuracy: 0.2204\n",
            "Epoch 122/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.6180 - accuracy: 0.2206\n",
            "Epoch 123/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 4.6239 - accuracy: 0.2203\n",
            "Epoch 124/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5883 - accuracy: 0.2232\n",
            "Epoch 125/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5858 - accuracy: 0.2237\n",
            "Epoch 126/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.5678 - accuracy: 0.2257\n",
            "Epoch 127/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.5771 - accuracy: 0.2237\n",
            "Epoch 128/200\n",
            "892/892 [==============================] - 81s 90ms/step - loss: 4.5721 - accuracy: 0.2258\n",
            "Epoch 129/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5683 - accuracy: 0.2252\n",
            "Epoch 130/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5611 - accuracy: 0.2280\n",
            "Epoch 131/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.5225 - accuracy: 0.2301\n",
            "Epoch 132/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5341 - accuracy: 0.2286\n",
            "Epoch 133/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5089 - accuracy: 0.2315\n",
            "Epoch 134/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5311 - accuracy: 0.2290\n",
            "Epoch 135/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5342 - accuracy: 0.2290\n",
            "Epoch 136/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.5210 - accuracy: 0.2330\n",
            "Epoch 137/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.4949 - accuracy: 0.2340\n",
            "Epoch 138/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.4819 - accuracy: 0.2341\n",
            "Epoch 139/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.4994 - accuracy: 0.2331\n",
            "Epoch 140/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.4885 - accuracy: 0.2351\n",
            "Epoch 141/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4854 - accuracy: 0.2367\n",
            "Epoch 142/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.4797 - accuracy: 0.2358\n",
            "Epoch 143/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.4690 - accuracy: 0.2390\n",
            "Epoch 144/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.4812 - accuracy: 0.2338\n",
            "Epoch 145/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4598 - accuracy: 0.2380\n",
            "Epoch 146/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.4558 - accuracy: 0.2410\n",
            "Epoch 147/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4524 - accuracy: 0.2405\n",
            "Epoch 148/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4635 - accuracy: 0.2387\n",
            "Epoch 149/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4228 - accuracy: 0.2406\n",
            "Epoch 150/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.4065 - accuracy: 0.2433\n",
            "Epoch 151/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4170 - accuracy: 0.2443\n",
            "Epoch 152/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4257 - accuracy: 0.2468\n",
            "Epoch 153/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.4029 - accuracy: 0.2452\n",
            "Epoch 154/200\n",
            "892/892 [==============================] - 79s 88ms/step - loss: 4.4113 - accuracy: 0.2451\n",
            "Epoch 155/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.4152 - accuracy: 0.2450\n",
            "Epoch 156/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.3989 - accuracy: 0.2463\n",
            "Epoch 157/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.3771 - accuracy: 0.2481\n",
            "Epoch 158/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.3851 - accuracy: 0.2470\n",
            "Epoch 159/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3759 - accuracy: 0.2471\n",
            "Epoch 160/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3858 - accuracy: 0.2486\n",
            "Epoch 161/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.3711 - accuracy: 0.2489\n",
            "Epoch 162/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.3599 - accuracy: 0.2533\n",
            "Epoch 163/200\n",
            "892/892 [==============================] - 81s 90ms/step - loss: 4.3520 - accuracy: 0.2514\n",
            "Epoch 164/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3549 - accuracy: 0.2525\n",
            "Epoch 165/200\n",
            "892/892 [==============================] - 81s 91ms/step - loss: 4.3557 - accuracy: 0.2511\n",
            "Epoch 166/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3518 - accuracy: 0.2520\n",
            "Epoch 167/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3310 - accuracy: 0.2543\n",
            "Epoch 168/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3240 - accuracy: 0.2563\n",
            "Epoch 169/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3346 - accuracy: 0.2551\n",
            "Epoch 170/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.3361 - accuracy: 0.2550\n",
            "Epoch 171/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.3097 - accuracy: 0.2588\n",
            "Epoch 172/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.3105 - accuracy: 0.2572\n",
            "Epoch 173/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.3127 - accuracy: 0.2575\n",
            "Epoch 174/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.3080 - accuracy: 0.2574\n",
            "Epoch 175/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.3136 - accuracy: 0.2591\n",
            "Epoch 176/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.3108 - accuracy: 0.2586\n",
            "Epoch 177/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.2940 - accuracy: 0.2571\n",
            "Epoch 178/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.2895 - accuracy: 0.2612\n",
            "Epoch 179/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.2730 - accuracy: 0.2635\n",
            "Epoch 180/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.2921 - accuracy: 0.2627\n",
            "Epoch 181/200\n",
            "892/892 [==============================] - 78s 87ms/step - loss: 4.2655 - accuracy: 0.2610\n",
            "Epoch 182/200\n",
            "892/892 [==============================] - 76s 85ms/step - loss: 4.2902 - accuracy: 0.2605\n",
            "Epoch 183/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.2785 - accuracy: 0.2631\n",
            "Epoch 184/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.2703 - accuracy: 0.2633\n",
            "Epoch 185/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.2550 - accuracy: 0.2651\n",
            "Epoch 186/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.2576 - accuracy: 0.2658\n",
            "Epoch 187/200\n",
            "892/892 [==============================] - 77s 86ms/step - loss: 4.2490 - accuracy: 0.2685\n",
            "Epoch 188/200\n",
            "892/892 [==============================] - 77s 87ms/step - loss: 4.2520 - accuracy: 0.2640\n",
            "Epoch 189/200\n",
            "892/892 [==============================] - 80s 90ms/step - loss: 4.2764 - accuracy: 0.2644\n",
            "Epoch 190/200\n",
            "892/892 [==============================] - 81s 91ms/step - loss: 4.2408 - accuracy: 0.2684\n",
            "Epoch 191/200\n",
            "892/892 [==============================] - 81s 90ms/step - loss: 4.2465 - accuracy: 0.2673\n",
            "Epoch 192/200\n",
            "892/892 [==============================] - 87s 97ms/step - loss: 4.2201 - accuracy: 0.2692\n",
            "Epoch 193/200\n",
            "892/892 [==============================] - 81s 91ms/step - loss: 4.2500 - accuracy: 0.2688\n",
            "Epoch 194/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.2165 - accuracy: 0.2698\n",
            "Epoch 195/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.2000 - accuracy: 0.2716\n",
            "Epoch 196/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.1939 - accuracy: 0.2714\n",
            "Epoch 197/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.2104 - accuracy: 0.2710\n",
            "Epoch 198/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.2196 - accuracy: 0.2717\n",
            "Epoch 199/200\n",
            "892/892 [==============================] - 80s 89ms/step - loss: 4.2028 - accuracy: 0.2720\n",
            "Epoch 200/200\n",
            "892/892 [==============================] - 79s 89ms/step - loss: 4.1833 - accuracy: 0.2736\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7fdcaf1910>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5UC8litfi6r"
      },
      "source": [
        "model.save(path+'model_3.h5')\n",
        "dump(tokenizer, open(path+'tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHDiCKZSiHmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e8cd35-bfae-43e7-d036-4cc165ce558c"
      },
      "source": [
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_words):\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        pred = model.predict(encoded, verbose=0)\n",
        "        yhat = np.argmax(pred,axis=1)\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "model = load_model(path+'model_3.h5')\n",
        "\n",
        "tokenizer = load(open(path+'tokenizer.pkl', 'rb'))\n",
        "\n",
        "init_text = \"Sukienka \"\n",
        "print(init_text + '\\n')\n",
        "\n",
        "generated = generate(model, tokenizer, seq_length, init_text, 15)\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sukienka \n",
            "\n",
            "artystki kosmetyczki czy artystki kosmetyczki czy zabrała ulubieńców artystki różnorodnych wynik wynik wynik artystki artystki\n"
          ]
        }
      ]
    }
  ]
}